{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations on indexing and retrieval\n",
    "<!-- We want to evalute:\n",
    "* The performance of different indexing and retrieval strategies, spanning sparse retrieval, classic dense embedding, and advanced retrieval model.\n",
    "\n",
    "* The influence of precise retrieval on the quality of LLM interpretation -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Models\n",
    "\n",
    "We consider the performance of the following 5 indexing and retrieval methods:\n",
    "\n",
    "1) BM-25, a lightweight sparse retrieval method without complex neural networks, ranking document segments based on the appearing frequency of query terms.\n",
    "\n",
    "2) all-MiniLM-L6, from SentenceTransformer, a prevalent dense embedding model, mapping sentences to a 384-dimensional dense vector space. \n",
    "\n",
    "3) all-mpnet-base, another widely utilized embedding model from SentenceTransformer, noted for its larger architecture and improved performance. \n",
    "\n",
    "4) text-embedding-3-large-model, the latest embedding model from OpenAI, with enhanced capability. \n",
    "\n",
    "5) ColBERT, an advanced retrieval model, relying on token-level embedding and fine-grained contextual late interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root directory\n",
    "root_dir = Path(os.path.abspath(\"\")).resolve().parents[1]\n",
    "sys.path.append(str(root_dir))\n",
    "# Change the working directory to the project root\n",
    "os.chdir(root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the configs for this demo\n",
    "DEMO_SIZE = 2\n",
    "res_dir = f\"experiment/retrieval/res/\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the retrieval experiments, utilizing the functional implementation provided within the `uda.utils` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Start fin on bm25 ===\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'Top-1': 0.7555555555555555, 'Top-5': 0.9333333333333333, 'Top-10': 0.9333333333333333, 'Top-20': 0.9333333333333333, 'Top-30': 0.9333333333333333}\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-1', 'Top-1': 0.7555555555555555, 'Top-5': 0.9333333333333333, 'Top-10': 0.9333333333333333, 'Top-20': 0.9333333333333333, 'Top-30': 0.9333333333333333}\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_161.pdf-1', 'Top-1': 0.631578947368421, 'Top-5': 0.9836065573770492, 'Top-10': 0.9836065573770492, 'Top-20': 0.9836065573770492, 'Top-30': 0.9836065573770492}\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_183.pdf-3', 'Top-1': 0.2857142857142857, 'Top-5': 1.0, 'Top-10': 1.0, 'Top-20': 1.0, 'Top-30': 1.0}\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_186.pdf-2', 'Top-1': 0.9411764705882353, 'Top-5': 0.9411764705882353, 'Top-10': 0.9411764705882353, 'Top-20': 0.9411764705882353, 'Top-30': 0.9411764705882353}\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-4', 'Top-1': 0.6666666666666666, 'Top-5': 0.7192982456140351, 'Top-10': 0.7192982456140351, 'Top-20': 0.7192982456140351, 'Top-30': 0.7192982456140351}\n",
      "=== Finish fin ===\n",
      "\n",
      "=== Start paper_tab on bm25 ===\n",
      "Retrieval-Match-Scores {'doc_name': '1809.01202', 'q_uid': '4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94', 'Top-1': 0.24107142857142858, 'Top-5': 0.6800595238095238, 'Top-10': 0.6800595238095238, 'Top-20': 0.6800595238095238, 'Top-30': 0.6800595238095238}\n",
      "=== Finish paper_tab ===\n",
      "\n",
      "=== Start paper_text on bm25 ===\n",
      "No human-anotated evidence for file 2001.03131 and qa_id 133eb4aa4394758be5f41744c60c99901b2bc01c\n",
      "Retrieval-Match-Scores {'doc_name': '2001.03131', 'q_uid': 'a778b8204a415b295f73b93623d09599f242f202', 'Top-1': 0.25757575757575757, 'Top-5': 0.8825757575757576, 'Top-10': 0.8825757575757576, 'Top-20': 0.8825757575757576, 'Top-30': 0.8825757575757576}\n",
      "=== Finish paper_text ===\n",
      "\n",
      "=== Start nq on bm25 ===\n",
      "Retrieval-Match-Scores {'doc_name': 'Hannah John-Kamen', 'q_uid': -6718102858366318183, 'Top-1': 0.23157894736842105, 'Top-5': 0.8105263157894737, 'Top-10': 0.8105263157894737, 'Top-20': 0.8105263157894737, 'Top-30': 0.8105263157894737}\n",
      "=== Finish nq ===\n",
      "\n",
      "=== Start feta on bm25 ===\n",
      "Retrieval-Match-Scores {'doc_name': 'Smallville', 'q_uid': 12844, 'Top-1': 0.7878787878787878, 'Top-5': 0.851619644723093, 'Top-10': 0.851619644723093, 'Top-20': 0.851619644723093, 'Top-30': 0.851619644723093}\n",
      "=== Finish feta ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from uda.utils import retrieve as rt\n",
    "from uda.utils import retrieve_exp as rt_exp\n",
    "from uda.utils import preprocess as pre\n",
    "import json\n",
    "\n",
    "DATASET_NAME_LIST = [\"fin\", \"paper_tab\", \"paper_text\", \"nq\", \"feta\"]\n",
    "RT_MODEL_LIST = [\"bm25\", \"all-MiniLM-L6-v2\", \"all-mpnet-base-v2\", \"openai\", \"colbert\"]\n",
    "# The procedure of complex models may be time-consuming, you can choose to run a sub-list of models and datasets\n",
    "DATASET_NAME_LIST = DATASET_NAME_LIST[:]\n",
    "RT_MODEL_LIST = RT_MODEL_LIST[:1]\n",
    "\n",
    "\n",
    "for DATASET_NAME in DATASET_NAME_LIST:\n",
    "    for RT_MODEL in RT_MODEL_LIST:\n",
    "        print(f\"=== Start {DATASET_NAME} on {RT_MODEL} ===\")\n",
    "        res_file = os.path.join(res_dir, f\"{DATASET_NAME}_{RT_MODEL}.jsonl\")\n",
    "        bench_json_file = pre.meta_data[DATASET_NAME][\"bench_json_file\"]\n",
    "        with open(bench_json_file, \"r\") as f:\n",
    "            bench_data = json.load(f)\n",
    "        doc_list = list(bench_data.keys())\n",
    "        for doc in doc_list[:1]:\n",
    "            pdf_path = pre.get_example_pdf_path(DATASET_NAME, doc)\n",
    "            if pdf_path is None:\n",
    "                continue\n",
    "            # Prepare the index for the document\n",
    "            collection_name = f\"{DATASET_NAME}_vector_db\"\n",
    "            collection = rt.prepare_collection(pdf_path, collection_name, RT_MODEL)\n",
    "            for qa_item in bench_data[doc]:\n",
    "                question = qa_item[\"question\"]\n",
    "                q_uid = qa_item[\"q_uid\"]\n",
    "                # Retrieve the contexts\n",
    "                contexts = rt.get_contexts(collection, question, RT_MODEL)\n",
    "                # Save the results\n",
    "                rt_exp.log_score(\n",
    "                    contexts, doc, q_uid, DATASET_NAME, res_file, bench_json_file\n",
    "                )\n",
    "            rt.reset_collection(collection_name, RT_MODEL)\n",
    "    print(f\"=== Finish {DATASET_NAME} ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the averaged retrieval matching scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== bm25 on fin =====\n",
      "   avg_1_score  avg_5_score  avg_10_score  avg_20_score\n",
      "0     0.672708     0.918458      0.918458      0.918458\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_avg_score(file_path):\n",
    "    with open(file_path,\"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        data_list=[json.loads(l) for l in lines]\n",
    "    df=pd.DataFrame(data_list)\n",
    "    avg_1_score=df[\"Top-1\"].mean()\n",
    "    avg_5_score=df[\"Top-5\"].mean()\n",
    "    avg_10_score=df[\"Top-10\"].mean()\n",
    "    avg_20_score=df[\"Top-20\"].mean()\n",
    "    res_df=pd.DataFrame({\"avg_1_score\":[avg_1_score],\"avg_5_score\":[avg_5_score],\"avg_10_score\":[avg_10_score],\"avg_20_score\":[avg_20_score]})\n",
    "    return res_df\n",
    "\n",
    "# rt_models=[\"bm25\",\"all-MiniLM-L6-v2\",\"all-mpnet-base-v2\",\"openai\",\"colbert\"]\n",
    "dataset_name=\"fin\"\n",
    "rt_model=\"bm25\"\n",
    "# relative path based on the project root\n",
    "res_file_name=f\"experiment/retrieval/res/{dataset_name}_{rt_model}.jsonl\" \n",
    "res_df=get_avg_score(res_file_name)\n",
    "print(f\"===== {rt_model} on {dataset_name} =====\")\n",
    "print(res_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of precise retrieval on the quality of LLM generation\n",
    "\n",
    "We use human-annotated evidence to evaluate how precise retrieval can affect LLM generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from uda.utils import retrieve as rt\n",
    "from uda.utils.parsing_exp import paper_raw_extract_table\n",
    "import numpy as np\n",
    "\n",
    "#  The contexts are not retrieved using embedding models\n",
    "#  They are directly extracted from the human-annotated evidence\n",
    "#  The following codes extract the direct evidence for various datasets\n",
    " \n",
    "def direct_fin(pdf_path, qa_item):\n",
    "    # fetch the target page\n",
    "    q_uid = qa_item[\"q_uid\"]\n",
    "    target_page = int(q_uid.split(\"page_\")[-1].split(\".pdf\")[0]) - 1\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        # read the pdf file of target page\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        pdf_page = reader.pages[target_page]\n",
    "        text = pdf_page.extract_text()\n",
    "        return text\n",
    "\n",
    "\n",
    "def direct_tat(qa_item):\n",
    "    # fetch the target page(s)\n",
    "    page_uid = qa_item[\"doc_page_uid\"]\n",
    "    page_path = f\"dataset/extended_qa_info/tat_doc_page_example/{page_uid}.pdf\"\n",
    "    if os.path.exists(page_path) is False:\n",
    "        raise Exception(f\"TAT Page not found: {page_path}\")\n",
    "    page_text = rt.extract_text_from_pdf(page_path)\n",
    "    return page_text\n",
    "\n",
    "\n",
    "def direct_paper_text(qa_item):\n",
    "    # fetch the direct evidence\n",
    "    evidence = qa_item[\"evidence\"]\n",
    "    contexts = []\n",
    "    for ev in evidence:\n",
    "        hi_evs = ev[\"highlighted_evidence\"]\n",
    "        for hi_ev in hi_evs:\n",
    "            if hi_ev not in contexts:\n",
    "                contexts.append(hi_ev)\n",
    "    return \"\\n\".join(contexts)\n",
    "\n",
    "\n",
    "def direct_paper_tab(pdf_path, qa_item):\n",
    "    # fetch the direct evidence\n",
    "    evidence = qa_item[\"evidence\"]\n",
    "    contexts = []\n",
    "    for ev in evidence:\n",
    "        hi_evs = ev[\"highlighted_evidence\"]\n",
    "        for hi_ev in hi_evs:\n",
    "            if ev.startswith(\"FLOAT SELECTED: Table\"):\n",
    "            # extract the evidential table chunk\n",
    "                table_name = ev.split(\"FLOAT SELECTED: \")[-1]\n",
    "                table_context= paper_raw_extract_table(pdf_path, table_name)\n",
    "                if table_context not in contexts:\n",
    "                    contexts.append(table_context)\n",
    "            elif hi_ev not in contexts:\n",
    "            # directly use the text evidence\n",
    "                contexts.append(hi_ev)\n",
    "    return \"\\n\".join(contexts)\n",
    "\n",
    "def direct_feta(pdf_path, qa_item):\n",
    "    # fetch the chunk that similar to the well-parsed table\n",
    "    table = qa_item[\"evidence\"][\"table_array\"]\n",
    "    well_parsed_table_text = \"\\n\".join([\" \".join(row) for row in table])\n",
    "    text = rt.extract_text_from_pdf(pdf_path)\n",
    "    chunks = rt.split_text(text,chunk_size=3000,overlap_size=500)\n",
    "    chunk_scores = []\n",
    "    for chunk in chunks:\n",
    "        score, _ = rt.char_lcs(well_parsed_table_text, chunk)\n",
    "        chunk_scores.append(score)\n",
    "    target_chunk_idx = int(np.argmax(chunk_scores))\n",
    "    return chunks[target_chunk_idx]\n",
    "\n",
    "\n",
    "def direct_nq(pdf_path, qa_item):\n",
    "    # fetch the chunks that similar to the paragraph-level long answer\n",
    "    long_answer = qa_item[\"answers\"][\"long_answer\"]\n",
    "    chunks = rt.split_text(rt.extract_text_from_pdf(pdf_path))\n",
    "    scores = []\n",
    "    for chunk in chunks:\n",
    "        score, _ = rt.word_lcs(long_answer, chunk)\n",
    "        scores.append(score)\n",
    "    target_chunk_idx = int(np.argmax(scores))\n",
    "    return chunks[target_chunk_idx]\n",
    "\n",
    "def get_direct_context(dataset_name, pdf_path, qa_item):\n",
    "    if dataset_name == \"fin\":\n",
    "        return direct_fin(pdf_path, qa_item)\n",
    "    elif dataset_name == \"tat\":\n",
    "        return direct_tat(qa_item)\n",
    "    elif dataset_name == \"paper_text\":\n",
    "        return direct_paper_text(qa_item)\n",
    "    elif dataset_name == \"paper_tab\":\n",
    "        return direct_paper_tab(pdf_path, qa_item)\n",
    "    elif dataset_name == \"nq\":\n",
    "        return direct_nq(pdf_path, qa_item)\n",
    "    elif dataset_name == \"feta\":\n",
    "        return direct_feta(pdf_path, qa_item)\n",
    "    else:\n",
    "        raise Exception(f\"Dataset name not found: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental Configurations\n",
    "\n",
    "res_dir = f\"experiment/retrieval/res/\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "\n",
    "DATASET_NAME_LIST = [\"fin\", \"tat\", \"feta\",  \"paper_text\", \"nq\", \"paper_tab\"]\n",
    "LOCAL_LLM_DICT = {\"llama-8B\": \"meta-llama/Meta-Llama-3-8B-Instruct\"}\n",
    "LLM_LIST = [\"gpt4\", \"llama-8B\"]\n",
    "\n",
    "# Sample a subset for faster demo\n",
    "DATASET_NAME_LIST = DATASET_NAME_LIST[:2]\n",
    "LLM_LIST = LLM_LIST[:1]\n",
    "DEMO_DOC_NUM = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the RAG procedure with human-annotated evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Start fin on gpt4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:46:44,360 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2015?', 'response': 'The answer is: 57.5%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'answers': {'str_answer': '57%', 'exe_answer': 0.57484}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:46:48,941 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2016?', 'response': 'The total long-term assets under supervision in 2016 are $983 billion, and the fixed income assets under supervision are $578 billion.\\n\\nTo calculate the percentage of fixed income assets in the total long-term assets under supervision for 2016:\\n\\n\\\\( \\\\frac{578}{983} \\\\times 100 = 58.8\\\\% \\\\)\\n\\nThe answer is: 58.8%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-1', 'answers': {'str_answer': '59%', 'exe_answer': 0.588}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:46:53,961 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total loans receivable gross in 2016 were loans backed by commercial real estate?', 'response': 'The total loans receivable, gross in 2016 were $50,181 million, and the loans backed by commercial real estate were $4,761 million.\\n\\nTo calculate the percentage:\\n\\n(Loans backed by commercial real estate / Total loans receivable, gross) * 100\\n= ($4,761 million / $50,181 million) * 100\\n= 9.48%\\n\\nThe answer is: 9.48%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_161.pdf-1', 'answers': {'str_answer': '9%', 'exe_answer': 0.09488}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:06,577 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of future minimum rental payments are due in 2018?', 'response': 'The total future minimum rental payments as of December 2016 are $1,941 million. The payments due in 2018 are $282 million.\\n\\nTo calculate the percentage due in 2018, we use the formula:\\n\\n(Annual payments due / Total future payments) * 100\\n\\nSo for 2018, it would be:\\n\\n($282 million / $1,941 million) * 100 = 14.53%\\n\\nThe answer is: 14.53%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_183.pdf-3', 'answers': {'str_answer': '15%', 'exe_answer': 0.14529}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:11,377 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'in millions , for 2016 , 2015 , and 2014 what was the total amount of common share repurchases?', 'response': 'The answer is: 2016 - 36.6 million, 2015 - 22.1 million, 2014 - 31.8 million', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_186.pdf-2', 'answers': {'str_answer': '90.1', 'exe_answer': 90.5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:12,859 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'of the total aus net inflows/ ( outflows ) for 2014 were fixed income asset inflows in connection with our acquisition of deutsche asset & wealth management 2019s stable value business greater than the liquidity products inflows in connection with our acquisition of rbs asset management 2019s money market funds?', 'response': 'The answer is: Yes', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-4', 'answers': {'str_answer': 'yes', 'exe_answer': 'yes'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:19,087 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what was the percentage change in the 5 year annual performance of the peer group stock from 2010 to 2011', 'response': 'The percentage change in the 5 year annual performance of the peer group stock from 2010 to 2011 is calculated as follows:\\n\\n((Value in 2011 - Value in 2010) / Value in 2010) * 100\\n\\nUsing the values from the report:\\n\\n((136.78 - 100.00) / 100.00) * 100 = 36.78%\\n\\nThe answer is: 36.78%', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-2', 'answers': {'str_answer': '8.3%', 'exe_answer': 0.08276}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:20,771 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'in 2010 , what was the cumulative total return of the s&p 500?', 'response': 'The answer is: 100.00', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-3', 'answers': {'str_answer': '30.69', 'exe_answer': 30.69}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:22,415 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'for the 2010 , what was the cumulative total return on jkhy?', 'response': 'The answer is: 100.00', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-1', 'answers': {'str_answer': '27.44', 'exe_answer': 27.44}}\n",
      "=== Finish fin ===\n",
      "\n",
      "=== Start tat on gpt4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:24,149 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': \"What is the difference between  Richard E. Belluzzo's total compensation as compared to Laura Black?\", 'response': 'The answer is: 92,500.00', 'doc': 'viavi-solutions-inc_2019', 'q_uid': 'f1b63dd3002720ebbffa8f36d852d351', 'answers': {'answer': 92500, 'answer_type': 'arithmetic', 'scale': ''}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:28,347 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'How much did Donald Colvin earn in cash?', 'response': 'The answer is: 97,500', 'doc': 'viavi-solutions-inc_2019', 'q_uid': 'd1a4ba4e1af05c60efa468f953a4f8ca', 'answers': {'answer': ['97,500'], 'answer_type': 'span', 'scale': ''}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:33,083 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'What is the percentage difference of the total compensation between Timothy Campos and Tor Braham?', 'response': \"The total compensation for Timothy Campos is $255,817 and for Tor Braham is $245,817. To calculate the percentage difference:\\n\\nDifference = $255,817 - $245,817 = $10,000\\n\\nPercentage difference = (Difference / Tor Braham's total compensation) * 100\\nPercentage difference = ($10,000 / $245,817) * 100 â‰ˆ 4.07%\\n\\nThe answer is: 4.07%\", 'doc': 'viavi-solutions-inc_2019', 'q_uid': '5cff0439063c6a496073ce4b55591d0b', 'answers': {'answer': 4.07, 'answer_type': 'arithmetic', 'scale': 'percent'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:35,502 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'What does cash and cash equivalents consist of?', 'response': 'The answer is: cash, checking accounts, money market accounts, and temporary investments with maturities of three months or less when purchased.', 'doc': 'inpixon_2019', 'q_uid': 'c97f3158996e35cf911eec06d020badc', 'answers': {'answer': ['cash, checking accounts, money market accounts and temporary investments with maturities of three months or less when purchased.'], 'answer_type': 'span', 'scale': ''}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:43,382 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'What was the average Professional and legal fees for 2018 and 2019?', 'response': 'The total Professional and legal fees for 2018 and 2019 are $545,000 and $242,000 respectively. To find the average, we add these two amounts together and divide by 2.\\n\\nAverage Professional and legal fees = ($545,000 + $242,000) / 2 = $787,000 / 2 = $393,500\\n\\nThe answer is: $393,500', 'doc': 'inpixon_2019', 'q_uid': 'd7d2b6427fb0f566a3ac2e90d842dd06', 'answers': {'answer': 393.5, 'answer_type': 'arithmetic', 'scale': 'thousand'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 10:47:48,447 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'How does the company account for options granted to employees?', 'response': 'The company accounts for options granted to employees by measuring the cost of services received in exchange for the award of equity instruments based upon the fair value of the award on the date of grant. The fair value of that award is then ratably recognized as an expense over the period during which the recipient is required to provide services in exchange for that award. Forfeitures of unvested stock options are recorded when they occur.\\n\\nThe answer is: By measuring the cost of services received in exchange for the award based on the fair value of the award on the grant date and recognizing it over the service period.', 'doc': 'inpixon_2019', 'q_uid': 'fde5598a4f22f1775d27e488f367cdb7', 'answers': {'answer': ['by measuring the cost of services received in exchange for the award of equity instruments based upon the fair value of the award on the date of grant.'], 'answer_type': 'span', 'scale': ''}}\n",
      "=== Finish tat ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from uda.utils import retrieve as rt\n",
    "from uda.utils import preprocess as pre\n",
    "import pandas as pd\n",
    "from uda.utils import llm\n",
    "from uda.utils import inference\n",
    "import json\n",
    "\n",
    "for DATASET_NAME in DATASET_NAME_LIST:\n",
    "    for LLM_MODEL in LLM_LIST:\n",
    "        print(f\"=== Start {DATASET_NAME} on {LLM_MODEL} ===\")\n",
    "        res_file = os.path.join(res_dir, f\"{DATASET_NAME}_{LLM_MODEL}_direct.jsonl\")\n",
    "\n",
    "        # If use the local LLM, initialize the model\n",
    "        if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "            llm_name = LOCAL_LLM_DICT[LLM_MODEL]\n",
    "            llm_service = inference.LLM(llm_name)\n",
    "            llm_service.init_llm()\n",
    "\n",
    "        # Load the benchmark data\n",
    "        bench_json_file = pre.meta_data[DATASET_NAME][\"bench_json_file\"]\n",
    "        with open(bench_json_file, \"r\") as f:\n",
    "            bench_data = json.load(f)\n",
    "\n",
    "        # Run experiments on the demo docs\n",
    "        doc_list = list(bench_data.keys())\n",
    "        for doc in doc_list[:DEMO_DOC_NUM]:\n",
    "            pdf_path = pre.get_example_pdf_path(DATASET_NAME, doc)\n",
    "            if pdf_path is None:\n",
    "                continue\n",
    "            for qa_item in bench_data[doc]:\n",
    "                question = qa_item[\"question\"]\n",
    "                # Directly retrieve the human-annotated evidence \n",
    "                context_text = get_direct_context(DATASET_NAME, pdf_path, qa_item)\n",
    "                # Create the prompt\n",
    "                llm_message = llm.make_prompt(question, context_text, DATASET_NAME, LLM_MODEL)\n",
    "                # Generate the answer\n",
    "                if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "                    response = llm_service.infer(llm_message)\n",
    "                elif LLM_MODEL == \"gpt4\":\n",
    "                    # Set up with your own GPT4 service using environment variables\n",
    "                    response = llm.call_gpt(messages=llm_message)\n",
    "                    if response is None:\n",
    "                        print(\"Make sure your gpt4 service is set up correctly.\")\n",
    "                        raise Exception(\"GPT4 service\")\n",
    "\n",
    "                # log the results\n",
    "                res_dict = {\"model\": LLM_MODEL, \"question\": question, \"response\": response, \"doc\": doc, \"q_uid\": qa_item[\"q_uid\"], \"answers\": qa_item[\"answers\"]}\n",
    "                print(res_dict)\n",
    "                with open(res_file, \"a\") as f:\n",
    "                    f.write(json.dumps(res_dict) + \"\\n\")\n",
    "\n",
    "    print(f\"=== Finish {DATASET_NAME} ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the accuracy of the generation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the generating performance with the human-annotated evidential contexts in this demo. \n",
    "\n",
    "To evaluate the performance with model-retrieved contexts, please refer to the basic end-to-end demo in [e2e.ipynb](../e2e/e2e.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact-match accuracy: 66.67\n"
     ]
    }
   ],
   "source": [
    "dataset_name=\"fin\"\n",
    "llm_model=\"gpt4\"\n",
    "res_file_name=f\"experiment/retrieval/res/{dataset_name}_{llm_model}_direct.jsonl\"\n",
    "\n",
    "from uda.eval.my_eval import eval_from_file\n",
    "eval_from_file(dataset_name, res_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
